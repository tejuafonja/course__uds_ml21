{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463e0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_circles, make_blobs\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458f410",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs) - Tutorial - 17.06.2021\n",
    "\n",
    "## Overview\n",
    "\n",
    "Hello! This is a practical tutorial. In this notebook, we are going to explore Support Vector Machines (SVMs) hands-on.  We have left a few `#TODO` where implementation is missing. Take a look and try to implement it yourself! HINT: There is a comprehensive scikit-learn documentation on SVMs [here](https://scikit-learn.org/stable/modules/svm.html#svm-classification). We will illustrate the implementation at the tutorial on Thursday. See you there!\n",
    "\n",
    "The main goals of this notebook are:\n",
    "\n",
    "- Learn how to use the plug-and-play models implemented in the Python package scikit-learn.\n",
    "- Explore the power of the parameter `C`.\n",
    "- Explore the power of different kernels with different parameters `gamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910af662",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "In this notebook we will be using the following three datasets:\n",
    "- Dataset A: 2-dimensional 2-classes isotropic Gaussian blobs. The samples for each class are generated from a gaussian distribution with a certain mean and standard deviation. This dataset is linearly separable.\n",
    "- Dataset B: 2-dimensional 2-classes isotropic Gaussian blobs. The samples for each class are generated from a gaussian distribution with a certain mean and standard deviation. This dataset is not linearly separable.\n",
    "- Dataset C: 2-dimensional 2-classes circles.  Samples for each class are generated from circles with a certain radius plus some noise. This dataset is not linearly separable.\n",
    "- Dataset D: 2-dimensional 3-classes isotropic Gaussian blobs. The samples for each class are generated from a gaussian distribution with a certain mean and standard deviation. This dataset is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a dataset\n",
    "def plot_dataset(X, y, title, axes=None):\n",
    "      if axes is None:\n",
    "        axes = plt.gca()\n",
    "      y_zero = y==0\n",
    "      y_one = y==1\n",
    "\n",
    "      axes.scatter(X[y_zero,0],X[y_zero,1],color = \"red\", label = \"Class 0\")\n",
    "      axes.scatter(X[y_one,0],X[y_one,1], color = \"blue\", label = \"Class 1\")\n",
    "      axes.set_title(title)\n",
    "      axes.set_xlabel(\"x1\")\n",
    "      axes.set_ylabel(\"x2\")\n",
    "      axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef50511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "np.random.seed(5)\n",
    "# Number of training samples\n",
    "n_samples = 100\n",
    "# Dataset A\n",
    "X_blobs1, y_blobs1 = make_blobs(n_samples=n_samples, centers=[(-1, -1), (1, 1)], cluster_std = 0.45)\n",
    "X_blobs1_test, y_blobs1_test = make_blobs(n_samples=1000, centers=[(-1, -1), (1, 1)], cluster_std = 0.45)\n",
    "# Dataset B\n",
    "X_blobs2, y_blobs2 = make_blobs(n_samples=2*n_samples, centers=[(-1, -1), (1, 1)], cluster_std = 0.8)\n",
    "X_blobs2_test, y_blobs2_test = make_blobs(n_samples=500, centers=[(-1, -1), (1, 1)], cluster_std = 0.8)\n",
    "# Dataset C\n",
    "X_circles, y_circles = make_circles(n_samples=n_samples,factor=.1, noise=.05)\n",
    "X_circles_test, y_circles_test = make_circles(n_samples=500,factor=.1, noise=.05)\n",
    "# Dataset D\n",
    "X_blobs3, y_blobs3 = make_blobs(n_samples=n_samples, centers=[(-1, -1), (1, 1), (2,-1)], cluster_std = 0.5)\n",
    "X_blobs3_test, y_blobs3_test = make_blobs(n_samples=500, centers=[(-1, -1), (1, 1), (2,-1)], cluster_std = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the datasets\n",
    "fig, ax = plt.subplots(1,4, figsize=(20,5))\n",
    "# 2 classes\n",
    "plot_dataset(X_blobs1, y_blobs1, title=\"Dataset A: linearly separable\", axes = ax[0])\n",
    "plot_dataset(X_blobs2, y_blobs2, title=\"Dataset B: not linearly separable\", axes = ax[1])\n",
    "plot_dataset(X_circles, y_circles, title=\"Dataset C: non-linear\", axes = ax[2])\n",
    "# 3 classes\n",
    "y_two = y_blobs3==2\n",
    "ax[3].scatter(X_blobs3[y_two,0], X_blobs3[y_two,1],color = \"green\", label = \"Class 2\")\n",
    "plot_dataset(X_blobs3, y_blobs3, title=\"Dataset C: non-linear\", axes = ax[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be061076",
   "metadata": {},
   "source": [
    "## How to\n",
    "Throughout this notebook we will be analyzing SVM classifiers with different configurations (i.e., parameters and kernels) by computing the test error of the classifier(s), plotting their classification boundary and outputting the decision function. Lets start with looking at the functions that perfom this:\n",
    "\n",
    "- Compute test error with a function called `test_error`. Use as input the ground truth (correct) labels `y_true` and the predicted labels, as returned by a classifier `y_pred`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22215ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error(y_true, y_pred):\n",
    "    #TODO\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d60c7f",
   "metadata": {},
   "source": [
    "- Plot the decision boundary of a trained 2D Support Vector Classifier (SVC) (`model`) with a function called `plot_svc_decision_function`. The function should also plot the margin and have the option to plot the support vectors (`plot_support`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b276aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:            #If no figure handle is provided, it opens the current figure\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)    #30 points in the grid axis\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)                 # We create a grid with the x,y coordinates defined above\n",
    "    \n",
    "    # From the grid to a list of (x,y) values. \n",
    "    # Check Numpy help for ravel()\n",
    "    \n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T \n",
    "    \n",
    "    \n",
    "    # get decision boundary\n",
    "    #TODO\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    #TODO\n",
    "\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        #TODO\n",
    "        \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0cc42",
   "metadata": {},
   "source": [
    "# 1. Linearly separable data\n",
    "We start with looking at dataset A. \n",
    "## Maximum Margin Hyperplane\n",
    "You have the following three hyperplanes/decision boundaries given. You can see them as three suggestions on how to take decisions on the given dataset. Can you tell, which suggestion is the best? How would you proceed to analyze them? Lets take a look at it together, step by step. \n",
    "\n",
    "A maximum margin hyperplane $(w, b)$ for a linearly separable set of training data $\\left(x_{i}, y_{i}\\right)_{i=1}^{n}$ is defined as\n",
    "$$\n",
    "\\max _{\\mathbf{w} \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}} \\min \\left\\{\\left\\|x-x_{i}\\right\\| \\mid\\langle w, x\\rangle+b=0, x \\in \\mathbb{R}^{d}, i=1, \\ldots, n\\right\\}\n",
    "$$\n",
    "where we optimize over all $(w, b)$ such that $y_{i}\\left(\\left\\langle w, x_{i}\\right\\rangle+b\\right)>0$.\n",
    "\n",
    "\n",
    "A linear classifier is determined by the weight vector ${w}$ and the offset $b$.\n",
    "$$\n",
    "\\hat{y}({x})=\\operatorname{sign}(\\langle{w}, {x}\\rangle+b)\n",
    "$$\n",
    "\n",
    "These are our three hyperplanes (classifiers) for the 2D dataset.\n",
    "- Classifier A: $w = (1, 1)$, $b = 0$\n",
    "- Classifier B: $w=(1, 0.3)$, $b= 0$\n",
    "- Classifier C: $w = (0.1, 1.6)$, $b = 0$\n",
    "\n",
    "We will now take a closer look at maximum magine hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59e6e",
   "metadata": {},
   "source": [
    "- 1.1 Lets begin with plotting, how the given hyperplanes split the training data. Create a function `get_decision boundary` that computes the decision boundary given weights $w$ and bias $b$ for a given vector `X`. Then print all three hyperplanes. What do you observe? Can you tell from this plot, which one is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bc381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, bias as given\n",
    "# TODO\n",
    "\n",
    "def get_decision_boundary(w,b, X):\n",
    "    # TODO\n",
    "\n",
    "# plot all three decision boundaries\n",
    "# TODO\n",
    "\n",
    "# plot dataset\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7688840",
   "metadata": {},
   "source": [
    "- 1.2 Assume a fourth candidate D shows up with $w=(2,2)$ and $b = 0$. Before you plot it, can you say, if it will be better or worse than the other candidates? Now plot it together with the other candidates, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, bias as given \n",
    "# TODO\n",
    "\n",
    "# plot decision boundary\n",
    "# TODO\n",
    "\n",
    "# plot dataset \n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b89db",
   "metadata": {},
   "source": [
    "- 1.3 How do we find the best classifier candidate? From the lecture, we know that a maximum margin hyperplane is a hyperplane which correctly classifies the data and has maximum distance/margin to the data. Can you draw around each decision boundary a *margin* with some width up to the nearest point? What do you observe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot decision boundaries\n",
    "# TODO\n",
    "\n",
    "# plot margins left and right, you can use linestyle='dashed'\n",
    "# TODO\n",
    "\n",
    "\n",
    "# plot datasets\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84172b01",
   "metadata": {},
   "source": [
    "- 1.4 Lets compute the test error of each hyperplane. Define first a function `clf_predict` that predicts the decisions for a given classifier: $\\hat{y}({x})=\\operatorname{sign}(\\langle{w}, {x}\\rangle+b)$. The function takes as input weight $w$ and bias $b$ of the hyperplane and features `X`. Use the `test_error` function you defined above to compute the test error for Classifiers A, B and C. Are the results in line with your observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_predict(w,b,X):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf30289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test error for A, B, C\n",
    "# TOODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bade7e",
   "metadata": {},
   "source": [
    "## Hard-margin Support Vector Machine\n",
    "We formalize a hard-margin SVM as\n",
    "\\begin{array}{l}\n",
    "\\min _{w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}} \\frac{1}{2}\\|w\\|^{2} \\\\\n",
    "\\text { subject to: } y_{i}\\left(\\left\\langle w, x_{i}\\right\\rangle+b\\right) \\geq 1, \\quad \\forall i=1, \\ldots, n\n",
    "\\end{array}\n",
    "\n",
    "The support vectors are the points on the margin, that is $\\langle w, x\\rangle+b=\\pm 1$.The area between the two supporting hyperplanes $\\{x \\mid\\langle w, x\\rangle+b=1\\}$ and $\\{x \\mid\\langle w, x\\rangle+b=-1\\}$ is called the margin.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4eccc0",
   "metadata": {},
   "source": [
    "- 1.5 Analyze the performance of a linear SVM. For simplicity you can use [this](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model. Use a default value of `C=1000`. Compute the test error and plot the classification boundary. How many support vectors do you find? Can you print them out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# fit model\n",
    "# TODO\n",
    "\n",
    "# compute test error\n",
    "# TODO\n",
    "\n",
    "# plot dataset\n",
    "# TODO\n",
    "\n",
    "# plot decision boundary\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18deabd9",
   "metadata": {},
   "source": [
    "1.6 In the lecture, we learned that modifications of the training points matter only if they fall into the margin / or on the wrong side of the decision boundary. Lets try that. Let us use dataset A. Find a datapoint for each of the following cases and augment the the dataset A with the datapoint. Analyze the performance of a linear SVM with `C=1000` on both the original and the augmented dataset. Compute the test error and plot the classification boundary with the support vectors.\n",
    "- POINT A: correctly classified, outside margin\n",
    "- POINT B: exactly on margin\n",
    "- POINT C: correctly classified, inside margin\n",
    "- POINT D: misclassified, inside margin\n",
    "- POINT E: misclassified, outside margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "\n",
    "# Fit model\n",
    "# TODO\n",
    "\n",
    "# Compute test error\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Points A, B, C, D, E:\n",
    "\n",
    "## Define point and concatenate to X_blobs1 and y_blobs1\n",
    "## TODO\n",
    "\n",
    "## fit model on augmented dataset\n",
    "## TODO\n",
    "\n",
    "## compute test error\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31511de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original dataset and the 5 different augmented datasets and decision boundaries\n",
    "# TODO\n",
    "\n",
    "# optional: colour the added point in green for visibility\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36039abf",
   "metadata": {},
   "source": [
    "# 2. Not linearly seperable data\n",
    "Previously we have defined a hard-margin SVM. We now move to dataset B and explore, why is this problematic and what a soft-margin SVM does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdde319",
   "metadata": {},
   "source": [
    "## Soft-margin Support Vector Machine\n",
    "We formalize a soft-margin SVM as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\mathbf{w} \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}, \\boldsymbol{\\xi} \\in \\mathbb{R}^{n}} & \\frac{1}{2}\\|\\mathbf{w}\\|^{2}+\\frac{C}{n} \\sum_{i=1}^{n} \\xi_{i} \\\\\n",
    "\\text { subject to: } & Y_{i}\\left(\\left\\langle\\mathbf{w}, \\mathrm{x}_{i}\\right\\rangle+b\\right) \\geq 1-\\xi_{i}, \\quad \\forall i=1, \\ldots, n, \\\\\n",
    "& \\xi_{i} \\geq 0, \\quad \\forall i=1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6ecda",
   "metadata": {},
   "source": [
    "- 1.7 What is the functionality of parameter `C`? Why did we choose in the previous example `C=1000`? Explore different values of the `C` parameter `[0.001, 0.01, 1, 10, 100, 1000]` for a linear SVM. Compute the test error and plot the classification boundary for each parameter value. Why did we not include `C=0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc23274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original dataset B\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the C values given above\n",
    "## fit model\n",
    "## TODO\n",
    "\n",
    "## compute test error\n",
    "## TODO\n",
    "\n",
    "## plot dataset\n",
    "## TODO\n",
    "\n",
    "## plot decision boundary\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d48f8f",
   "metadata": {},
   "source": [
    "# 3. Non-linear problems\n",
    "Until now, we have seen linear separable and non-separable problems. We solved with a linear SVM with a soft margin. A linear SVM uses a kernel: $\\left\\langle x, x^{\\prime}\\right\\rangle$. We now move to dataset C and non-linear problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb972da8",
   "metadata": {},
   "source": [
    "- 3.1 Analyze the performance of a linear SVM with value `C = 20` on dataset C. Compute the test error and plot the classification boundary and support vectors. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e850ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model \n",
    "# TODO\n",
    "\n",
    "# compute the test error\n",
    "# TODO\n",
    "\n",
    "# plot the dataset\n",
    "# TODO\n",
    "\n",
    "# plot the decision boundary\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2957c",
   "metadata": {},
   "source": [
    "## Radial Basis Function (RBF) Kernel\n",
    "We can use an SVM for non-linear problems by choosing a kernel that fits to our data. The radial basis function (RBF) kernel is defined as $$\\exp \\left(-\\gamma\\left\\|x-x^{\\prime}\\right\\|^{2}\\right),$$ where $\\gamma$ is specified by parameter gamma and must be greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8cdd8",
   "metadata": {},
   "source": [
    "- 3.2  Before we move to solving this non-linear problem with an SVM, lets compute a radial basis function centered on the middle clump: `r = np.exp(-(X ** 2).sum(1))`. Visualize this extra data dimension using a three-dimensional plot. HINT: Running the notebook live, you will be able to rotate the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc970ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code the radial basis function given above\n",
    "# TODO\n",
    "\n",
    "# concatenate it as third dimension to the dataset\n",
    "# TODO\n",
    "\n",
    "# plot the data with a 3D plot, colour one class in red, the other in blue\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23ca1e",
   "metadata": {},
   "source": [
    "- 3.3 Train a no-linear SVM with a RBF kernel. Set `C=1` and explore different values of the `gamma` parameter `[0.01, 0.1, 1, 10, 100]`. Compute the test error and plot the classification boundary. Set `gamma=1` and explore different values of the `C` parameter `[0.01, 0.1, 1, 10, 100]`. Compute the test error and plot the classification boundary. - What does a parameter `C<0`, `C=0` and `C>1` mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384dc5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each value in gamma\n",
    "\n",
    "## fit the model \n",
    "## TODO\n",
    "\n",
    "## compute the test error\n",
    "## TODO\n",
    "\n",
    "## plot the dataset\n",
    "## TODO\n",
    "\n",
    "## plot the decision boundary\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each value in C\n",
    "\n",
    "## fit the model \n",
    "## TODO\n",
    "\n",
    "## compute the test error\n",
    "## TODO\n",
    "\n",
    "## plot the dataset\n",
    "## TODO\n",
    "\n",
    "## plot the decision boundary\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc85617",
   "metadata": {},
   "source": [
    "# 4. Choosing Parameters\n",
    "One of the limitations of SVMs is that there is the complexity parameter `C` - and for non-linear kernels a parameter previously called `gamma` - that must be found using a hold-out method such as cross-validation (CV). Lets take a look at this together.\n",
    "- Use a 5-fold CV (via `cross_val_score`, see [here](https://scikit-learn.org/stable/modules/cross_validation.html)) to adjust both `C` and `gamma` parameters of a RBF SVM for not linearly seperable data. Explore values of `C` in `[0.01, 0.01, 0.1, 1, 10, 100, 1000]`, and define the range of `gamma` as: `[0.125, 0.25, 0.5, 1, 2, 4, 8])/D`, being `D` the data dimension. Note that this definition of the `gamma` values is used alleviate the influence of the data dimension in the definition of the RBF kernel. Print mean and standard deviation of the scores. Which model provides you the best results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combination of C and gamma\n",
    "\n",
    "## compute cross validation score \n",
    "## TODO\n",
    "\n",
    "## print mean and standard deviation\n",
    "## TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bb521",
   "metadata": {},
   "source": [
    "HINT: For further information on the parameters and their trade-off, take a look [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012d4a5",
   "metadata": {},
   "source": [
    "# 5. Different Kernels, Multiple Classes \n",
    "In addition to the linear and RBF kernel, scikit-learn also implements e.g. the following kernels\n",
    "- polynomial: $\\left(\\gamma\\left\\langle x, x^{\\prime}\\right\\rangle+r\\right)^{d}$, where $d$ is specified by parameter `degree`, $r$ by `coef0`.\n",
    "-  sigmoid: $\\tanh \\left(\\gamma\\left\\langle x, x^{\\prime}\\right\\rangle+r\\right)$,  where  $r$  is specified by `coef0`.\n",
    "\n",
    "We will take a brief look at what how their decision boundaries look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7121c",
   "metadata": {},
   "source": [
    "- Lets move to a multi-class dataset D and take a look at the decision boundaries of the different kernels. Analyze the performance of an SVM with `linear`, `rbf`(gamma=0.7),`poly`(with `degree=3`, `gamma='auto'`) and `sigmoid`(`gamma='auto'`) kernels. For the sake of simplicity, you can use a default value of `C = 1`. Compute the test error and plot the classification boundary. What does the gamma mean for each of the kernels? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here: https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    # Create a mesh of points to plot in\n",
    "    # from \n",
    "    \n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    #Plot the decision boundaries for a classifier.\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5790a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the four SVC models\n",
    "## TODO\n",
    "\n",
    "# for each model\n",
    "## fit\n",
    "## TODO\n",
    "\n",
    "## compute test error\n",
    "## TODO\n",
    "\n",
    "\n",
    "# from here: https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html\n",
    "fig, sub = plt.subplots(2, 2,figsize=(15,15))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X_blobs3[:, 0], X_blobs3[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, errors, ax in zip(models, titles, errors, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y_blobs3, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('x_1')\n",
    "    ax.set_ylabel('x_2')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title + \", Error \" + str(er))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b4fcb",
   "metadata": {},
   "source": [
    "HINT: Not enough kernels to choose from? Interested in a DIY-kernel? Check [this](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py) out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c1dea",
   "metadata": {},
   "source": [
    "# 6. Further Considerations (self-study)\n",
    "The following considerations are not study focus of this notebook. \n",
    "- unbalanced classes: [here](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py)\n",
    "- weighted samples: [here](https://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html#sphx-glr-auto-examples-svm-plot-weighted-samples-py)\n",
    "- one-versus-rest multi-class approach: [here](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html#sphx-glr-auto-examples-svm-plot-iris-svc-py)\n",
    "- SVMs on hand written digits: [here](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
