{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LAB_RNN_STUDENTS.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gTANWiDGt6em"},"source":["\n","# RNNs for Time Series Prediction\n","\n","\n","In this tutorial  we will deploy a simple probabilistic model using a Recurrent Neural Network (RNN) to infer the probability distribution of a time-series. Given a set of signals $\\mathcal{D}=\\{X^1,X^2,\\ldots,X^N\\}$ where \n","$$X^i = [X_0^i,X_1^i,\\ldots,X_T^i]$$\n","is the $i$-th signal, we will train a probabilistic model of the form\n","$$p(X|X_0) = \\prod_{t=0}^T p(X_t|X_{t-1},X_{:t-1})$$\n","where $X_{:t-1}$ is the signal up to time and $X_0$ is the first step of the signal, which we assume to be $X_0 \\sim   \\mathcal{N}\\left( 0, \\sigma^2\\right)$. Additionally, we assume  each factor is a Gaussian distribution with fixed variance $\\sigma^2$ and mean given by **the ouput of a RNN** with input $X_{t-1}$. For any time $t$ the signal up to time $t-1$, that is $X_{:t-1}$,  is embedded through the **RNN** state $\\mathbf{h}_{t-1}$. Hence, the conditional probability $p(X_t|X_{t-1},X_{:t-1})$ is given by:\n","\n","\n","$$p(X_t|X_{t-1},X_{:t-1}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$\n","\n","During training, for $t=1,\\ldots,T$, we will sample $\\hat{X}_t$ from $p(X_t|X_{t-1},X_{:t-1})$ and minimize the average squared loss $\\frac{1}{T}\\sum_{t=1}^T(X_t-\\hat{X}_t)^2$. Then we average again for all signals in the training set. Note that during training we **feed the RNN with the true values of the signal** $X^i = [X_0^i,X_1^i,\\ldots,X_T^i]$. \n","\n","We would like to give credit to Prof. Pablo M. Olmos (Universidad Carlos III) whose notebook inspired ours.\n","\n","**IMPORTANT NOTE:** The exercises throught the notebook are indicated as follows:\n","> **Exercise**:\n","\n","These exercises ask you to complete missing parts in the code. Missing code can be located whenever you find the following comment:\n","```\n","# YOUR CODE HERE\n","```"]},{"cell_type":"code","metadata":{"id":"og_pDXv6tzSX"},"source":["import torch\n","from torch import nn\n","from torch import optim\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-ILGEyuyPjQ"},"source":["## Part I. Create a synthetic databse\n","\n","We will generate $N$ target signals of length $T$ time steps. We generate each signal as one realization of the following autoregressive model\n","\\begin{align}\n","X_{t}=c+\\sum_{i=1}^{p} \\varphi_{i} X_{t-i}+\\varepsilon_{t}\n","\\end{align}\n","\n"]},{"cell_type":"code","metadata":{"id":"orf7aMhKyQwi"},"source":["N = 1000 # Number of signals\n","\n","T = 200\n","\n","c = 0\n","# weights previous p=3 time steps\n","phi_1 = 1\n","\n","phi_2 = -1\n","\n","phi_3 = 1\n","\n","sigma = 1\n","\n","X = np.zeros([N,T])\n","\n","np.random.seed(23)\n","\n","# initialize the first 4 time steps randomly with contributions from previous time steps as applicable\n","X[:,0] = c + np.random.randn(N,)*np.sqrt(sigma)\n","\n","X[:,1] = c + phi_1 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n","\n","X[:,2] = c + phi_1 * X[:,1] + phi_2 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n","\n","X[:,3] = c + phi_1 * X[:,2] + phi_2 * X[:,1] + phi_3 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n","\n","t = 4\n","\n","while (t<T):\n","\n","    X[:,t] = c + phi_1 * X[:,t-1] + phi_2 * X[:,t-2] + phi_3 * X[:,t-3] + np.random.randn(N,)*np.sqrt(sigma)\n","    \n","    t +=1\n","    \n","\n","# Create targets\n","# for 0 <= i <= T-1, X[:,i] is the currently observed value and Y[:,i] is the value we want to predict\n","\n","Y = X[:,1:] # all time steps but the first (targets to predict)\n","X = X[:,:-1] # all time steps but the last\n","\n","T -=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sY8dPTb4yvQC"},"source":["The goal of the RNN will be to predict the value of the signal in the next time step $t$ given the current observation at time $t-1$. Note that the noise in the model\n","\n","$$p(X_t|X_{t-1},X_{:t-1}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$\n","\n","will simply introduce an error that will prevent the model from overfitting. \n","\n","Let's plot one of the signals versus the *target*, which is the same signal but shifted to the right ..."]},{"cell_type":"code","metadata":{"id":"V3CeQnuWyp__"},"source":["# Plot the signal \n","plt.figure(figsize=(8,5))\n","plt.title(\"whole series\")\n","plt.plot(np.arange(T), X[1,:T], 'r.--', label='input, x',ms=10) # x\n","plt.plot(np.arange(T), Y[1,:T], 'b.-', label='target, y',ms=10) # y\n","\n","plt.legend(loc='best')\n","\n","# Plot the signal (20 first steps)\n","plt.figure(figsize=(8,5))\n","plt.title(\"first 20 steps\")\n","plt.plot(np.arange(20), X[1,:20], 'r.--', label='input, x',ms=10) # x\n","plt.plot(np.arange(20), Y[1,:20], 'b.-', label='target, y',ms=10) # y\n","\n","plt.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrQ30U9Cy6bx"},"source":["## Part II. RNN\n","\n","Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, which takes in a number of parameters:\n","* **input_size** - the number of expected features in the input $x$\n","* **hidden_dim** - the number of features in the hidden state $h$\n","* **n_layers** - the number of recurrent layers. If you're stacking up multiple recurrent layers (i.e.,  `num_layers>1`) one after the other we're talking about **stacked RNNs**. For example, setting `num_layers=2` would mean stacking two RNNs together, with the second RNN taking in outputs of the first RNN and computing the final results. \n","\n","This is an example of a stacked RNN\n","\n","<img src=\"https://yiyibooks.cn/__src__/wizard/nmt-tut-neubig-2017_20180721165003_deleted/img/6-5.jpg\" width=\"40%\"> \n","\n","\n","If you take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn), you will see that `nn.RNN` only provides the actual computation of the hidden states along time\n","\\begin{align}\n","h_{t}=g \\left(W_{i h} x_{t}+b_{i h}+W_{h h} h_{(t-1)}+b_{h h}\\right)\n","\\end{align}\n","\n","If we want the output of our RNN (in our case $x_{t + 1}$) to have a different size than the state $h_t$, then we'll add a last, fully-connected layer to get the output size that we want. For simplicity, **the input to this dense layer is the state $h_t$ of the RNN, i.e. $x_{t + 1} = f(h_t)$**.\n","\n","You have to pay special attention to the dimensions of the input/output tensors of the RNN. **Check the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn)**.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pocmMOkwzHWx"},"source":["The following class implements the network architecture where \n","- An input signal with features of dimension `input_size` is processed by a RNN. As a result, we obtain a sequence of states $\\mathbf{h}_{t}$, from $t=1$ to $t=T$.\n","- We process each state with a linear layer to estimate the output signal (of dimension `output_size`) at time $t$ from $\\mathbf{h}_{t}$. \n","- We add Gaussian noise with variance `sigma` to the output of the linear layer.\n","\n","\n","**Remark:** Do not confuse the PyTorch implementation of a RNN `nn.RNN` with our implementation `RNN`.\n","\n","> **Exercise**: complete the following code. Understand all steps, particularly those in the `forward` method."]},{"cell_type":"code","metadata":{"id":"5tM4eJKryzJq"},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma):\n","        \n","        # input size -> Dimension of the input signal\n","        # outpu_size -> Dimension of the output signal\n","        # hidden_dim -> Dimension of the rnn state\n","        # n_layers -> Number of recurrent layers. If >1, we are using a stacked RNN.\n","        \n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        \n","        self.input_size = input_size\n","        \n","        self.sigma = sigma\n","\n","        # define a RNN with specified parameters\n","        # SUGGEST: batch_first=True means that the batch size is the first component of the input tensor's shape\n","        # batch_first=True means that the first dimension of the input will be the batch_size\n","        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, \n","                          nonlinearity='relu',batch_first=True)\n","        \n","        # last, fully-connected layer\n","        self.fc1 = # YOUR CODE HERE\n","\n","    def forward(self, x, h0=None):\n","        \n","        '''\n","        About the shape of the different tensors ...:\n","        \n","        - Input signal x has shape (batch_size, seq_length, input_size)\n","        - The initialization of the RNN hidden state h0 has shape (n_layers, batch_size, hidden_dim).\n","          If None value is used, internally it is initialized to zeros.\n","        - The RNN output has shape (batch_size, seq_length, hidden_size). This output is the RNN's state unfolded in time  \n","\n","        '''\n","        batch_size = x.size(0) # Number of signals N\n","        seq_length = x.size(1) # T\n","        \n","        # get RNN outputs\n","        # r_out is the sequence of states\n","        # hidden is just the last state (we will use it for forecasting)\n","        r_out, hidden = # YOUR CODE GOES HERE \n","        \n","        # shape r_out to be (seq_length, hidden_dim) #UNDERSTANDING THIS POINT IS IMPORTANT!!\n","        # passing -1 as parameter means that the sequence length is inferred from r_out        \n","        r_out = r_out.reshape(-1, self.hidden_dim) \n","        \n","        output = self.fc1(r_out)\n","        \n","        noise = torch.randn_like(output)*sigma\n","        \n","        output += noise\n","        \n","        # reshape back to temporal structure\n","        output = output.reshape([-1,seq_length,1])\n","        \n","        return output, hidden\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvP0WSVPzNAS"},"source":["> **Exercise:** Instantiate the object RNN implemented above with the right parameters for our problem. Use `hidden_dim=32`, `n_layers=1` and `sigma=1`. "]},{"cell_type":"code","metadata":{"id":"cMMoEmGIzJyA"},"source":["test_rnn = # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ufzmkqd1zbl9"},"source":["In the following code, we compute the model output using the `forward` method. Note that we use an all zero initial state.\n","\n","> **Exercise**: Complete the following code. What are the dimensions of variables `h` and `o`? How are these dimensions related to the number of signals, hidden state of the RNN and signal duration?"]},{"cell_type":"code","metadata":{"id":"YTpM9uWYzO6l"},"source":["X_in = torch.Tensor(X).view([-1,T,1])\n","\n","o,h = # YOUR CODE HERE\n","\n","\n","dim_h = h.shape \n","dim_o = o.shape \n","\n","print(f\"dim_h={dim_h}\")\n","\n","print(f\"dim_o={dim_o}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ecut5yxXzsOW"},"source":["> **Exercise:** Complete the code for the following class, which extends `RNN` to include a training method.  Store the values of the loss for every training iteration.\n","\n","Note that there is no mini-batch, we process all signals for every SGD iteration. You are free employ the mini-batch training functionality."]},{"cell_type":"code","metadata":{"id":"dLkgSLlPzdgI"},"source":["class RNN_extended(RNN):\n","    \n","    \n","    def __init__(self, num_data_train, num_iter, sequence_length,\n","                 input_size, output_size, hidden_dim, n_layers, sigma, lr=0.001):\n","        \n","        super().__init__(input_size, output_size, hidden_dim, n_layers,sigma) \n","        \n","        self.hidden_dim = hidden_dim\n","        \n","        self.sequence_length = sequence_length\n","        \n","        self.num_layers = n_layers # number of recurrent layers\n","        \n","        self.lr = lr # learning rate\n","        \n","        self.num_train = num_data_train # number of training signals\n","        \n","        self.optim = optim.Adam(self.parameters(), self.lr) # optimizer\n","        \n","        self.num_iter = num_iter # number of training iterations\n","        \n","        self.criterion = nn.MSELoss() # loss function\n","                \n","        # A list to store the loss evolution along training\n","        \n","        self.loss_during_training = [] \n","        \n","           \n","    def trainloop(self,x,y):\n","        '''\n","        x: input signals shaped (n_samples, n_tsteps, n_features)\n","        y: target signal shaped (n_samples, n_tsteps, 1)\n","        '''\n","        # SGD Loop\n","        \n","        for e in range(int(self.num_iter)):\n","            # reset the gradient\n","            self.optim.zero_grad() \n","                \n","            x = torch.Tensor(x).view()  # YOUR CODE HERE: arrange x to the required shape\n","\n","            y = torch.Tensor(y).view()  # YOUR CODE HERE: arrange y to the required shape\n","\n","                            \n","            # YOUR CODE HERE\n","            # 1. Forward the signal\n","            # 2. compute the loss and add it to running_loss\n","            # 3. Perform backward pass\n","\n","            \n","            # This code helps to avoid vanishing exploiting gradients in RNNs\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n","                \n","            self.optim.step()\n","            \n","            if(e % 50 == 0): # Every 10 iterations\n","\n","                print(\"Iteration %d. Training loss: %f\" %(e,self.loss_during_training[-1]))                "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9PQqyFUPzzxh"},"source":["> **Exercise:** Using only the first 100 time steps of every signal, train the RNN for 100 SGD iterations. Use `hidden_dim=32`, `n_layers=1` and `sigma=1`. Recall that the target signal is stored in the variable `Y`."]},{"cell_type":"code","metadata":{"id":"_1AOacw0zxtN"},"source":["T_train = 100\n","my_rnn = # YOUR CODE HERE: instatiate extended class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_GbYUHrz4d-"},"source":["# YOUR CODE HERE: run the training loop on the first T_train time steps\n","my_rnn.trainloop(X[:,:T_train],Y[:,:T_train])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3dR2PMyHz9Nw"},"source":["> **Exercise:** Plot the loss for every training iteration."]},{"cell_type":"code","metadata":{"id":"tHglkrZGz6Ar"},"source":["# YOUR CODE HERE\n","plt.plot(my_rnn.loss_during_training,label='Training Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7RJfVXY0CQv"},"source":["> **Exercise:** For $50 \\leq t \\leq 100$ predict the target signal using the network. Plot the input signal, target signal and predicted signal"]},{"cell_type":"code","metadata":{"id":"SBmK7e5kz_Ba"},"source":["# We first evaluate the model for the N signals up to time T_train = 100\n","X_in = torch.Tensor(X[:,:T_train]).view([N,T_train,1]) \n","\n","o,h =  # YOUR CODE HERE\n","\n","output_rnn = o.detach().numpy().reshape([N,-1])\n","\n","offset = 50\n","\n","signal = 0 # index of the signal to plot from 0 to N-1 (you can play with this)\n","\n","# Plot the first training signal and the target\n","plt.figure(figsize=(8,5))\n","plt.plot(np.arange(T_train-offset,T_train,1), X[signal,T_train-offset:T_train], 'r.--', label='input, x',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), Y[signal,T_train-offset:T_train], 'b.-', label='target, y',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), output_rnn[signal,T_train-offset:T_train], 'k.-', label='RNN output',ms=10) # y\n","\n","\n","plt.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h__XHYnB0J4R"},"source":["\n","\n","Observe that the prediction is pretty good! The RNN model has clearly learnt the dynamics of the dataset. In the previous experiment, we  fed the RNN model with the **true** values of the signal, In other words, we use  $\\hat{X}_t$ at time $t$ to predict $\\hat{X}_{t+1}$ and we loop for all the values of the signal.\n","\n","Using the model we have just trained, let's now do **forecasting**. Namely, we feed the RNN the output that we predicted and we do this recursively for as long as we want. This represents **sampling** from the probabilistic model \n","\n","\n","\n","$$p(X|X_{:T_{train}}) = \\prod_{t=T_{train}}^T \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$\n","\n","\n","To forecast, we have to repeatedly call the `forward` method, feeding the output and state from the last call as the signal and state to the next  `forward` call. The following code would do the job:"]},{"cell_type":"code","metadata":{"id":"UNQFSvJD0EqA"},"source":["# We take the last RNN output\n","current_input = o[:,-1,:].view([N,1,1]) #Note that current input only contains one observation for each of the N signals\n","# We take the last RNN state\n","current_state = h\n","# Will hold the forecasted signals\n","forecast_rnn = np.zeros([N,T-T_train])\n","\n","for t in range(T-T_train):\n","    \n","    # ... and feed them as input and initial state\n","    \n","    current_input,current_state = my_rnn.forward(current_input,current_state)\n","    \n","    forecast_rnn[:,t] = current_input.detach().numpy().reshape([-1,])\n","    \n","# stack predictions from true data and forecasts\n","final_rnn_reconstruct = np.hstack([output_rnn,forecast_rnn])\n","\n","# We plot the signal and the target before and after forecasting\n","\n","plt.plot(np.arange(0,T-1,1), Y[signal,:-1].reshape([-1]), 'b.-', label='target, y',ms=10) \n","plt.plot(np.arange(0,T-1,1), final_rnn_reconstruct[signal,:-1], 'g.-', label='RNN output',ms=10) \n","plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--')\n","plt.legend()\n","\n","print('Between t=0 and t=100, we feed the real values')\n","print('From t=100, we feed the estimated values (forecasting)')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Z7pHJt_0W4D"},"source":["Observe that, during forecasting, sometimes the model quickly diverges from the target. That is **expected**, because we are sampling from the generative model and it is likely that we do not get exactly the same sample! Particularly in RNN, since they have short memory.\n","\n","## Part III. LSTM\n","\n","LSTMs were designed to mitigate the short lifetime of memory in a traditional RNN. Intuitively, they capture long-term associations by adding more flexibility to the state. A LSTM cell can decide to which extent the current signal changes the current state and how much of the current state is forgotten depending on the current signal.\n","\n","\n","You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n","\n","```python\n","lstm = nn.LSTM(input_size, n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","```\n","\n","where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. If **stacked LSTMs (`n_layers>1`) are used** we can automatically add dropout between LSTM layers with te parameter `dropout` with a specified probability.\n","\n","In this task we'll use a modified version of the previous architecture obtained by replacing the RNN layers by LSTM layers. The RNN from the previous section had difficulties forcasting the signal, we would hope that we can address this issue with the more elaborate memory provided by LSTMs.\n","\n","\n","**Remark:** Do not confuse the PyTorch implementation of a LSTM `nn.LSTM` with our implementation `LSTM`.\n","\n","> **Exercise:** Complete the code for the following two classes "]},{"cell_type":"code","metadata":{"id":"G0KRwL1P0QXc"},"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma,drop_prob):\n","        \n","        # input size -> Dimension of the input signal\n","        # outpusize -> Dimension of the output signal\n","        # hidden_dim -> Dimension of the rnn state\n","        # n_layers -> If >1, we are using a stacked RNN\n","        \n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        \n","        self.input_size = input_size\n","        \n","        self.sigma = sigma\n","\n","        # define an RNN with specified parameters\n","        # batch_first=True means that the first dimension of the input will be the batch_size\n","        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        # add a last, fully-connected layer\n","        self.fc1 = # YOUR CODE HERE\n","\n","    def forward(self, x, h0=None, valid=False):\n","        \n","        '''\n","        About the shape of the different tensors ...:\n","        \n","        - Input signal x has shape (batch_size, seq_length, input_size)\n","        - The initialization of the LSTM hidden state is a tuple, containing two tensors of dimensions\n","          (n_layers, batch_size, hidden_dim) each. The first tensor represents the LSTM hidden state \n","          cell states. We can use the None value so internally they are initialized with 0s.\n","        - The LSTM output shape is (batch_size, seq_length, hidden_size) \n","\n","        valid: flag; if true perform forward pass in evaluation mode if false use training mode (e.g. use Dropout)\n","        '''\n","        \n","        if(valid):\n","            self.eval()\n","        else:\n","            self.train()\n","        \n","        batch_size = x.size(0) # Number of signals N\n","        seq_length = x.size(1) # T\n","        \n","        # get RNN outputs\n","        # r_out is the sequence of states\n","        # hidden is just the last state (we will use it for forecasting)\n","    \n","        r_out, hidden = # YOUR CODE HERE\n","        \n","        # shape r_out to be (seq_length, hidden_dim) # UNDERSTANDING THIS POINT IS IMPORTANT!!        \n","        r_out = r_out.reshape(-1, self.hidden_dim) \n","        \n","        output = self.fc1(r_out)\n","        \n","        noise = torch.randn_like(output)*sigma\n","        \n","        output += noise\n","        \n","        # reshape back to temporal structure\n","        output = output.reshape([-1,seq_length,1])\n","        \n","        return output, hidden\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGBQvYwV0fkw"},"source":["class LSTM_extended(LSTM):\n","        \n","    def __init__(self, num_data_train, num_iter, sequence_length,\n","                 input_size, output_size, hidden_dim, n_layers, sigma, drop_prob=0.3, lr=0.001):\n","        \n","        super().__init__(input_size, output_size, hidden_dim, n_layers,sigma,drop_prob) \n","        \n","        self.hidden_dim = hidden_dim\n","        \n","        self.sequence_length = sequence_length\n","        \n","        self.num_layers = n_layers\n","        \n","        self.lr = lr #Learning Rate\n","        \n","        self.num_train = num_data_train #Number of training signals\n","        \n","        self.optim = optim.Adam(self.parameters(), self.lr)\n","        \n","        self.num_iter = num_iter\n","        \n","        self.criterion =    # YOUR CODE HERE     \n","        \n","        # A list to store the loss evolution along training\n","        \n","        self.loss_during_training = [] \n","        \n","           \n","    def trainloop(self,x,y):\n","        '''\n","        x: signals shaped (n_samples, n_time_steps, n_features)\n","        y: targets shaped (n_samples, n_time_steps)\n","        '''\n","        # SGD Loop\n","        \n","        for e in range(int(self.num_iter)):\n","        \n","            self.optim.zero_grad() \n","\n","            # adjust shape of x and y    \n","            x = torch.Tensor(x).view()  #YOUR CODE HERE \n","\n","            y = torch.Tensor(y).view()  #YOUR CODE HERE \n","\n","            # YOUR CODE HERE\n","            # 1. Forward the signal\n","            # 2. compute the loss and add it to running_loss\n","            # 3. Perform backward pass\n","\n","            \n","            # This code helps to avoid vanishing exploiting gradients in RNNs\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n","                \n","            self.optim.step()\n","            \n","            if(e % 50 == 0): # Every 10 iterations\n","\n","                print(\"Iteration %d. Training loss: %f\" %(e,self.loss_during_training[-1]))                "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQUF5NfP0j93"},"source":["> **Exercise:** Train the LSTM model for 500 iterations using the first 100 values of each signal. Use `hidden_dim=32`, `n_layers=1` and `sigma=1`. Recall that the target signal is stored in the variable `Y`.\n","Note that with only one layer, the dropout probability parameter does not play any role (you will get a warning actually).\n"]},{"cell_type":"code","metadata":{"id":"STcJB1fz0iMO"},"source":["\n","my_lstm = # YOUR CODE HERE "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBn4i1NbbyAh"},"source":["my_lstm.trainloop(X[:,:T_train],Y[:,:T_train]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJD_UnMOcGOK"},"source":["plt.plot(my_lstm.loss_during_training,label='Training Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONMG590ZcJE8"},"source":["> **Exercise:**  For $0 \\leq t \\leq T_{train}$ predict the next value of the signal. Plot one input signal, predicted signal and target signal.\n"]},{"cell_type":"code","metadata":{"id":"S8BiIxswcJeu"},"source":["# We first evaluate the model for the N signals up to time T_train = 100\n","X_in = torch.Tensor(X[:,:T_train]).view([N,T_train,1]) \n","\n","o,h =  # YOUR CODE HERE\n","\n","output_lstm = o.detach().numpy().reshape([N,-1])\n","\n","offset = 50\n","\n","signal = 6 # From 1 to N (you can play with this)\n","\n","# Plot the first training signal and the target\n","plt.figure(figsize=(8,5))\n","plt.plot(np.arange(T_train-offset,T_train,1), X[signal,T_train-offset:T_train], 'r.--', label='input, x',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), Y[signal,T_train-offset:T_train], 'b.-', label='target, y',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), output_lstm[signal,T_train-offset:T_train], 'k.-', label='LSTM output',ms=10) # y\n","\n","\n","plt.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg893CQQcQkr"},"source":["> **Exercise:**  For $T_{train} \\leq t < T$ forecast the signals given the prediction at time $T_{train}$ and the state at time $T_{train}$. Plot the input signal, target signal, RNN prediction and forecast and the LSTM prediction and forecast. Discuss your findings."]},{"cell_type":"code","metadata":{"id":"izgYi1i4cQ69"},"source":["# We take the last RNN output \n","current_input = o[:,-1,:].view([N,1,1]) #Note that current input only contains one observation for each of the N signals\n","# We take the last RNN state\n","current_state = h\n","\n","forecast_lstm = np.zeros([N,T-T_train])\n","\n","for t in range(T-T_train):\n","    \n","    # ... and feed them as input and initial state\n","    \n","    current_input,current_state = # YOUR CODE HERE \n","    \n","    forecast_lstm[:,t] = current_input.detach().numpy().reshape([-1,])\n","    \n","final_lstm_reconstruct = np.hstack([output_lstm,forecast_lstm])\n","\n","# We plot the signal and the target before and after forecasting\n","\n","signal = 6\n","\n","plt.plot(np.arange(0,T-1,1), Y[signal,:-1].reshape([-1]), 'b.-', label='target, y',ms=10) \n","plt.plot(np.arange(0,T-1,1), final_lstm_reconstruct[signal,:-1], 'g.-', label='LSTM output',ms=10) \n","plt.plot(np.arange(0,T-1,1), final_rnn_reconstruct[signal,:-1], 'r-', label='RNN output',ms=10) \n","plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--')\n","plt.legend()\n","\n","print('Between t=0 and t=100, we feed the real values')\n","print('From t=100, we feed the estimated values (forecasting)')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g07AqXbScVkK"},"source":["You should observe that the LSTM is able to keep track of the real signal during forecasting for a longer period of time."]},{"cell_type":"markdown","metadata":{"id":"AnEEYwWIcYkZ"},"source":["> **Exercise:** Repeat the last exercise with a LSTM with 3 LSTM layers.\n","1. Train the model with signals up to time $T_{train}$.\n","2. Predict the target for $0 \\leq t < T_{train}$.\n","3. Forecast the target for $T_{train} \\leq t < T$ given the last prediction and state.\n","4. Plot the input signal, target and predicted/forecasted signal.\n"]},{"cell_type":"code","metadata":{"id":"GvZh85PGcV8K"},"source":["my_lstm3 = # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VtnIZoBcdWV"},"source":["my_lstm3.trainloop(X[:,:T_train],Y[:,:T_train])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfyJtdc3ckyh"},"source":["# We first evaluate the model for the N signals up to time T_train = 1000\n","X_in = torch.Tensor(X[:,:T_train]).view([N,T_train,1]) \n","\n","o,h = my_lstm3.forward(X_in) \n","\n","output_lstm3 = o.detach().numpy().reshape([N,-1])\n","\n","offset = 50\n","\n","signal = 6 # From 1 to N (you can play with this)\n","\n","# Plot the first training signal and the target\n","plt.figure(figsize=(8,5))\n","plt.plot(np.arange(T_train-offset,T_train,1), X[signal,T_train-offset:T_train], 'r.--', label='input, x',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), Y[signal,T_train-offset:T_train], 'b.-', label='target, y',ms=10) # x\n","plt.plot(np.arange(T_train-offset,T_train,1), output_lstm3[signal,T_train-offset:T_train], 'k.-', label='LSTM3 output',ms=10) # y\n","\n","\n","plt.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZXztbfccr50"},"source":["# We take the last RNN output \n","current_input = o[:,-1,:].view([N,1,1]) #Note that current input only contains one observation for each of the N signals\n","# We take the last RNN state\n","current_state = h\n","\n","forecast_lstm3 = np.zeros([N,T-T_train])\n","\n","for t in range(T-T_train):\n","    \n","    # ... and feed them as input and initial state\n","    \n","    current_input,current_state = # YOUR CODE HERE \n","    \n","    forecast_lstm3[:,t] = current_input.detach().numpy().reshape([-1,])\n","    \n","final_lstm3_reconstruct = np.hstack([output_lstm3,forecast_lstm3])\n","\n","# We plot the signal and the target before and after forecasting\n","\n","signal = 6\n","\n","plt.plot(np.arange(0,T-1,1), Y[signal,:-1].reshape([-1]), 'b.-', label='target, y',ms=10) \n","plt.plot(np.arange(0,T-1,1), final_lstm_reconstruct[signal,:-1], 'g.-', label='LSTM output',ms=10)\n","plt.plot(np.arange(0,T-1,1), final_lstm3_reconstruct[signal,:-1], 'k.-', label='LSTM3 output',ms=10)\n","plt.plot(np.arange(0,T-1,1), final_rnn_reconstruct[signal,:-1], 'r-', label='RNN output',ms=10) \n","plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--')\n","plt.legend()\n","\n","print('Between t=0 and t=100, we feed the real values')\n","print('From t=100, we feed the estimated values (forecasting)')"],"execution_count":null,"outputs":[]}]}