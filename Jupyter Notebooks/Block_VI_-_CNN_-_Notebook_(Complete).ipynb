{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVvCaKkfoBus"
   },
   "source": [
    "\n",
    "# Chapter I: Convolutional NNs for CIFAR 10\n",
    "\n",
    "In this tutorial  we will use a data set of (small) natural images known as  [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). Our goal is to present i) how CNN layers are used in Pytorch, and ii) evaluate the performance of a simple CNN over this dataset.\n",
    "\n",
    "A big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!! Also, we would like to give credit to Prof. Pablo M. Olmos (Universidad Carlos III) whose notebook inspired ours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps4bTQChoNDC"
   },
   "source": [
    "**IMPORTANT NOTE:** In this notebook I show you how to speed up NN training using Graphical Processing Units (GPUs). To make sure you use a Google Colaboratory server equipped with a GPU, go to `Edit` --> `Notebook Settings` --> Select GPU in `Hardware Accelerator`. \n",
    "\n",
    "**IMPORTANT NOTE 2:** The exercises throught the notebook are indicated as follows:\n",
    "> **Exercise**:\n",
    "\n",
    "These exercises ask you to complete missing parts in the code. Missing code can be located whenever you find the following comment:\n",
    "```\n",
    "# YOUR CODE HERE\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1626344552666,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "0_1o1jSZoi2p",
    "outputId": "57bb2056-c27e-4319-fe74-2e1468289410"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://pytorch.org/tutorials/_images/cifar10.png\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1626344602297,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "WaZetq9LouHR"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IarR5JvSoy2k"
   },
   "source": [
    "## Part I. Download CIFAR10 with `torchvision`\n",
    "\n",
    "The code below will download the MNIST dataset, then create training and test datasets for us. It is mostly the same code we used to download MNIST in the previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1626344828704,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "zhtp_bvtov0c",
    "outputId": "7b3194d1-230f-4000-9862-2f73ffb90aca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1626344882545,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "_aPJNdxho1wj",
    "outputId": "a2435be6-3879-4450-facd-13c3608563ad"
   },
   "outputs": [],
   "source": [
    "traindata = iter(trainloader)\n",
    "\n",
    "images, labels = next(traindata)\n",
    "\n",
    "print(f\"batch size: {images.shape}\")\n",
    "print(f\"image size: {images[1].shape}\")\n",
    "print(f\"max: {images.max()} min: {images.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1626344917216,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "-S1PK4-Lo3rF"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize to plot\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1626344922624,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "wIbksZJso46f",
    "outputId": "a09433e7-61bc-4344-ae3e-14ac519c7135"
   },
   "outputs": [],
   "source": [
    "imshow(utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW1v5ArTo91F"
   },
   "source": [
    "> **Exercise:** Create a validation set using the 20% of train images\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1626344994698,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "HVbGMLrqo6Kb"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import copy\n",
    "\n",
    "validloader = copy.deepcopy(trainloader)  # Creates a copy of the object \n",
    "\n",
    "#We take the first 40k images for training\n",
    "trainloader.dataset.data = trainloader.dataset.data[:40000,:,:,:]\n",
    "trainloader.dataset.targets = trainloader.dataset.targets[:40000]\n",
    "\n",
    "#And the rest for validation\n",
    "validloader.dataset.data = validloader.dataset.data[40000:,:,:]\n",
    "validloader.dataset.targets = validloader.dataset.targets[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZTIyk5npD3N"
   },
   "source": [
    "## Part II. Implement Lenet 5\n",
    "\n",
    "Our first goal is to implement the LeNet 5 CNN network, first published in November 1998. See the original paper [here](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1626345085581,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "Q7UqjGnppBFA",
    "outputId": "9140b4ba-c889-46d6-be32-459cdc67c3ff"
   },
   "outputs": [],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/800/1*lvvWF48t7cyRWqct13eU0w.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAt0sYV9pIjj"
   },
   "source": [
    "In the CNN network above we have 2 convolutional layers with the following properties:\n",
    "\n",
    "- ReLU activation functions are used as non-linear functions\n",
    "- Maxpooling with $2\\times 2$ kernels is used to reduce the spatial dimension in both layers.\n",
    "- $5\\times 5$ convolutional filters are used. Stride is 1.\n",
    "- After the second convolutional layer, three dense layers are stacked. \n",
    "\n",
    "Note that FMNIST images are $28\\times28$ (instead of $32\\times32$), so some of the spatial dimensions in the image above are different.\n",
    "\n",
    "> **Exercise:** Complete the following code that defines the above CNN. But first read the [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/nn.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1626346219637,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "6akbVH2PpG9Q"
   },
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    def __init__(self,dimx,nlabels): #Nlabels will be 10 in our case\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional layer (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, \n",
    "                               kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # convolutional layer (sees 12x12x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)\n",
    "\n",
    "        # Max pool layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = nn.Linear(400,120)\n",
    "\n",
    "        self.linear2 = nn.Linear(120,84)\n",
    "\n",
    "        self.linear3 = nn.Linear(84,10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
    "        \n",
    "        # Spatial dimension of the Tensor at the output of the 2nd CNN\n",
    "        self.final_dim = int(((dimx-4)/2-4)/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through the CNN operations\n",
    "        #YOUR CODE HERE\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x) # shape : [batch_size, 16, 5, 5]\n",
    "        # Flatten the tensor into a vector of appropiate dimension using self.final_dim\n",
    "        x = x.view(-1, 16 * self.final_dim**2)\n",
    "        # Pass the tensor through the Dense Layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.logsoftmax(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLXk0EbFpY3E"
   },
   "source": [
    "Now the network is defined, by now you should know how to move forward by your own!!\n",
    "\n",
    "> **Exercise:** Extend the class to incorporate a training method, to evaluate the both the validation and train losses and to evaluate the classification performance in a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626346815662,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "q0H09wH3pQxr"
   },
   "outputs": [],
   "source": [
    "class Lenet5_extended(Lenet5):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,nlabels,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,nlabels)  \n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = []\n",
    "    \n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:              \n",
    " \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images)\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step() \n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            \n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():            \n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    out = self.forward(images)\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "                    \n",
    "\n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f, Time per epoch: %f seconds\" \n",
    "                      %(e+1,self.loss_during_training[-1],self.valid_loss_during_training[-1],\n",
    "                       (time.time() - start_time)))\n",
    "\n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "\n",
    "                probs = self.forward(images)\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GASpsB2pfSA"
   },
   "source": [
    "> **Exercise:** Train the model for 5 epochs, plot the train/validation loss during training, and compute the train and validation performance. It will take some time!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97338,
     "status": "ok",
     "timestamp": 1626346935594,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "aayltpJ3pdz4",
    "outputId": "057c935e-86cf-457f-abf9-addcfebee792"
   },
   "outputs": [],
   "source": [
    "my_CNN = Lenet5_extended(dimx=32,nlabels=10,epochs=5,lr=1e-3)\n",
    "\n",
    "my_CNN.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 16055,
     "status": "ok",
     "timestamp": 1626346970394,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "ycfbcQgCpkeD",
    "outputId": "731391f0-be4b-448c-8905-1362b3f7bd57"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_CNN.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVMwjentpmsb"
   },
   "source": [
    "## Part III. GPU-based training\n",
    "\n",
    "As you noticed, training became excessively slow. The network is already quite deep and gradient evaluation becomes a heavy operation. \n",
    "\n",
    "PyTorch, along with pretty much every other deep learning framework, uses [CUDA](https://developer.nvidia.com/cuda-zone) to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using `model.to('cuda')`. You can move them back from the GPU with `model.to('cpu')` which you'll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, we will compare how long it takes to perform a forward and backward pass with and without a GPU.\n",
    "\n",
    "You can write device agnostic code which will automatically use CUDA if it's enabled like so:\n",
    "```python\n",
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "...\n",
    "\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "input = data.to(device)\n",
    "model = MyModule(...).to(device)\n",
    "```\n",
    "\n",
    "> **Exercise:** Complete the following class, which implements the CNN training and validation using a GPU (if possible). \n",
    "\n",
    "**Note: Google Colab Recommended**. When running the notebook in Google Colab, make sure you first to `Edit -- Notebook settings` and **select a GPU as Hardware accelerator.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1626347282388,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "3vF4WcV2pnBW"
   },
   "outputs": [],
   "source": [
    "class Lenet5_extended_GPU(Lenet5):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,nlabels,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,nlabels)  \n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = []\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:\n",
    "                \n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device)  \n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images)\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step()\n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "\n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():            \n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    # Move input and label tensors to the default device\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)                    \n",
    "                    \n",
    "                    out = self.forward(images)\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "                    \n",
    "\n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f, Time per epoch: %f seconds\" \n",
    "                      %(e+1,self.loss_during_training[-1],self.valid_loss_during_training[-1],\n",
    "                       (time.time() - start_time)))\n",
    "                \n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device) \n",
    "                probs = self.forward(images)\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72327,
     "status": "ok",
     "timestamp": 1626347362286,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "r9XdzL98pwe0",
    "outputId": "813dd0ad-ce84-4240-93fa-04b003745752"
   },
   "outputs": [],
   "source": [
    "my_CNN_GPU = Lenet5_extended_GPU(dimx=32,nlabels=10,epochs=5,lr=1e-3)\n",
    "\n",
    "my_CNN_GPU.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 12885,
     "status": "ok",
     "timestamp": 1626347377564,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "8KlcPrjDp2Lg",
    "outputId": "60bb3cba-86f5-48cc-d552-36476c561a07"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_CNN_GPU.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN_GPU.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN_GPU.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN_GPU.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSSxDDq4p3up"
   },
   "source": [
    "With a GPU, you will see that the time per epoch decreases significantly.  Using GPUs is a must for lage-scale deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFb-Gmojp691"
   },
   "source": [
    "## Part IV. Regularize the network and compare with a MLP\n",
    "\n",
    "> **Exercise**: Now that you know how to train the CNN network, your goals are:\n",
    "> - Check that the CNN is able to overfit\n",
    "> - Regularize the network with both early stopping and dropout. In my experience, it is more efficient to include the dropout layers in between the final MLP layers, rather than in between convolutional layers. Note that once you include dropout, it will take more epochs to converge. The more dropout layers, the more epochs typically you have to run. For this exercise, run at least 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558543,
     "status": "ok",
     "timestamp": 1626348042200,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "hg84A5HWp4PG",
    "outputId": "ee918c31-de6c-475f-e2e8-6d581f9041cd"
   },
   "outputs": [],
   "source": [
    "# We simply run more epochs of the above network\n",
    "\n",
    "my_CNN_GPU = Lenet5_extended_GPU(dimx=32,nlabels=10,epochs=40,lr=1e-3)\n",
    "my_CNN_GPU.trainloop(trainloader,validloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 12642,
     "status": "ok",
     "timestamp": 1626348054834,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "tv1t9t6-p-oO",
    "outputId": "ad113dc8-beb8-49de-90b4-eaf7538e26b0"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_CNN_GPU.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN_GPU.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN_GPU.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN_GPU.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626348203761,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "fWpdGv4HqGie"
   },
   "outputs": [],
   "source": [
    "class Lenet5_Drop(nn.Module):\n",
    "    def __init__(self,dimx,nlabels,prob): #Nlabels will be 10 in our case\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional layer (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, \n",
    "                               kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # convolutional layer (sees 12x12x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)\n",
    "        \n",
    "        # Max pool layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = nn.Linear(400,120)\n",
    "        \n",
    "        self.linear2 = nn.Linear(120,84)\n",
    "        \n",
    "        self.linear3 = nn.Linear(84,10)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        # Spatial dimension of the Tensor at the output of the 2nd CNN\n",
    "        self.final_dim = int(((dimx-4)/2-4)/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through the CNN operations\n",
    "        x = self.conv1(x) #YOUR CODE HERE\n",
    "        x = self.relu(x) \n",
    "        x = self.pool(x)\n",
    "        #x = self.dropout(x) \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x) \n",
    "        # Flatten the tensor into a vector\n",
    "        x = x.view(-1, 16 * self.final_dim**2)\n",
    "        # Pass the tensor through the Dense Layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.logsoftmax(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1626348205021,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "jPR4a_sDqNGO"
   },
   "outputs": [],
   "source": [
    "class Lenet5_extended_GPU_Drop(Lenet5_Drop):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,nlabels,prob,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,nlabels,prob)  \n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = []\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:\n",
    "                \n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device)  \n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images)\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step()\n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            \n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                self.eval()\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    # Move input and label tensors to the default device\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)                    \n",
    "                    \n",
    "                    out = self.forward(images)\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "\n",
    "            # set model back to train mode\n",
    "            self.train()\n",
    "\n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f, Time per epoch: %f seconds\" \n",
    "                      %(e + 1,self.loss_during_training[-1],self.valid_loss_during_training[-1],\n",
    "                       (time.time() - start_time)))\n",
    "                \n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        self.eval()\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device) \n",
    "                probs = self.forward(images)\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "            self.train()\n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283453,
     "status": "ok",
     "timestamp": 1626348494376,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "U11CdY1EqRNk",
    "outputId": "d5b1d2ff-e3ad-480e-d0fb-420d627c8507"
   },
   "outputs": [],
   "source": [
    "my_CNN_GPU_Drop = Lenet5_extended_GPU_Drop(dimx=32,nlabels=10,prob=0.5,epochs=20,lr=1e-3)\n",
    "my_CNN_GPU_Drop.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 12911,
     "status": "ok",
     "timestamp": 1626348507278,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "__OaB-PnqS4K",
    "outputId": "1475730f-af00-4e53-8f55-d0f82d87f13e"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_CNN_GPU_Drop.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN_GPU_Drop.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN_GPU_Drop.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN_GPU_Drop.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-KGRiyRqUeG"
   },
   "source": [
    "### Train an MLP to compare the performance (Optional)\n",
    "\n",
    "Train an MLP with 3-4 layers to compare the performance. Take into account that the input image has three color maps. If you stuck it into a vector, then the input dimension is 3x32x32 = 3072. An alternative is to compute the average between the three. Alternatively, you could use only one color map, or the three of them ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PleM9M-9qUIa"
   },
   "outputs": [],
   "source": [
    "class MLPdrop(nn.Module):\n",
    "    def __init__(self,dimx,hidden1,hidden2,hidden3,nlabels,prob): #Nlabels will be 10 in our case\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output1 = nn.Linear(dimx,hidden1)\n",
    "        \n",
    "        self.output2 = nn.Linear(hidden1,hidden2)\n",
    "        \n",
    "        self.output3 = nn.Linear(hidden2,hidden3)\n",
    "        \n",
    "        self.output4 = nn.Linear(hidden3,nlabels)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)   \n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)        \n",
    "        x = self.output2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)        \n",
    "        x = self.output3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)        \n",
    "        x = self.output4(x)\n",
    "        x = self.logsoftmax(x) #YOUR CODE HERE\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK549yOLqZjG"
   },
   "outputs": [],
   "source": [
    "class MLPdrop_extended(MLPdrop):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,hidden1,hidden2,hidden3,nlabels,prob,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,hidden1,hidden2,hidden3,nlabels,prob)  #To initialize `MLP`!\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = [] \n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # set model back to train mode\n",
    "        self.train()\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:              \n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step()\n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            \n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad(): \n",
    "                \n",
    "                # set model to evaluation mode\n",
    "                self.eval()\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    out = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "                    \n",
    "            # set model back to train mode\n",
    "            self.train()\n",
    "                    \n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f\" \n",
    "                      %(e + 1,self.loss_during_training[-1],self.valid_loss_during_training[-1]))\n",
    "\n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # set model to evaluation mode\n",
    "            self.eval()\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "\n",
    "                probs = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRfRMGDtqcDu"
   },
   "outputs": [],
   "source": [
    "my_MLP_drop = MLPdrop_extended(dimx=3072,hidden1=256,hidden2=128,hidden3=64,nlabels=10,epochs=30,lr=1e-3,prob=0.2)\n",
    "\n",
    "my_MLP_drop.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD_7b0Yjqe9H"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_MLP_drop.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_MLP_drop.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_MLP_drop.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_MLP_drop.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erCQSmXzqlB9"
   },
   "source": [
    "# Chapter II: Batch Normalization \n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). \n",
    "\n",
    "> It's called **batch** normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current *batch*.\n",
    "\n",
    "We will first analyze the effect of Batch Normalization (BN) in a simple NN with dense layers. Then you will be able to incorportate BN into the CNN that you designed in Chapter I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtwc9gmQrJVc"
   },
   "source": [
    "## Part I. Batch Normalization in PyTorch<a id=\"implementation_1\"></a>\n",
    "\n",
    "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
    "\n",
    "The following cells import the packages we need in the notebook and load the MNIST dataset to use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1626348750801,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "BEviPzQXqtZ6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "12e82b5fc40a494e8d441b02ada487b9",
      "1824ce4041514aff9523f4e4fbea0163",
      "45e89804b91b4ca4ba1ba9eb61c30987",
      "10ebd6941c274e7f8584cf75a79b6f4f",
      "7cb7bed1ba6b4f3188d4139d3b496b51",
      "36641a81159e42da8bd64d75af6b907f",
      "4c2c4f1597a3406dae7e8f92db7a28de",
      "bd8b2fdc2cdb46afa41cf3057dcc08d3",
      "9fc878b6c47b43fd824560215e6e54db",
      "b8aa27ae454140f6b7bb6fa5767985bf",
      "cb20c5d14cfa4e6b9a4807222a55b7f5",
      "fd1eb28cc4b141f58ec7e455d55685c1",
      "8d275b58bf5d41099b10fa29e2cb89b0",
      "836267d7b47e43149da45f05fedcf6ee",
      "3fcf45bc3193468d8a2938550b26999a",
      "316d38faeacc4dfea6c77bb8441039e1",
      "9d8cbf9151d34fb9833a155a4e78e651",
      "c686fec736044af3a676b9ac879b2830",
      "084989368adb44b3a337749b1eb855c9",
      "e5de62858f644f719a507050101a4a3d",
      "fc00377c262c4b7ab3d3bbbbd7c8bcd7",
      "b7bf1a83c39f441a955eea59b16699ec",
      "312521b46a2a4bb4bc7987f0d9b56aeb",
      "a070fec2b8ff4509a27bc1c9f2653a63",
      "ce333287af5545b598496f5be757ecff",
      "19a2f818ac9a4e66a00e281179081d5d",
      "77050df183934b04a5ed583f87075837",
      "09ca9aa3b64d4f2aa3d33f45008383c3",
      "1a1bbad96bef4540864980f74f767887",
      "7125df1f44634c059d563601ad8219e2",
      "a47a133f144a424d83375a32c7cb5636",
      "056f77ec13a8494a939307d1dd9ccd94"
     ]
    },
    "executionInfo": {
     "elapsed": 274127,
     "status": "ok",
     "timestamp": 1626349025347,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "Cygb5zIwrhp2",
    "outputId": "a18d0cd9-e449-4cf4-f5d8-340bc98f3ca0"
   },
   "outputs": [],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training  data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QTv_NhyrlZW"
   },
   "source": [
    "### Neural network classes\n",
    "\n",
    "The following class, `MLP`, allows us to create identical neural networks **with and without batch normalization** to compare. We are defining a simple NN with **two dense layers** for classification; this design choice was made to support the discussion related to batch normalization and not to get the best classification accuracy.\n",
    "\n",
    "Two importants points about BN:\n",
    "\n",
    "- We use PyTorch's [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d). This is the function you use to operate on linear layer outputs; you'll use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) for 2D outputs like filtered images from convolutional layers. \n",
    "- We add the batch normalization layer **before** calling the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1626349030238,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "g6Qklp0grjav"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dimx,hidden1,hidden2,nlabels,use_batch_norm): #Nlabels will be 10 in our case\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        self.output1 = nn.Linear(dimx,hidden1)\n",
    "        \n",
    "        self.output2 = nn.Linear(hidden1,hidden2)        \n",
    "        \n",
    "        self.output3 = nn.Linear(hidden2,nlabels)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden1)\n",
    "            \n",
    "            self.batch_norm2 = nn.BatchNorm1d(hidden2)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output1(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output2(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm2(x)        \n",
    "        x = self.relu(x)\n",
    "        x = self.output3(x)\n",
    "        x = self.logsoftmax(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRIgHOTTrsmE"
   },
   "source": [
    "> **Exercise:** \n",
    "> \n",
    "> - Create a validation set with the 20% of training set\n",
    "> - Extend the class above to incorporate a training method where both training and validation losses are computed, and a method to evaluate the classification performance on a given set\n",
    "\n",
    "**Note:** As we do with Dropout, for BN we have to call the methods `self.eval()` and `self.train()` in both validation and training. Setting a model to evaluation mode is important for models with batch normalization layers!\n",
    "\n",
    ">* Training mode means that the batch normalization layers will use **batch** statistics to calculate the batch norm. \n",
    "* Evaluation mode, on the other hand, uses the estimated **population** mean and variance from the entire training set, which should give us increased performance on this test data!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1626349038138,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "5lJaWggfrqHM"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "validloader = copy.deepcopy(trainloader)  # Creates a copy of the object \n",
    "\n",
    "#We take the first 45k images for training\n",
    "trainloader.dataset.data = trainloader.dataset.data[:48000,:,:]\n",
    "trainloader.dataset.targets = trainloader.dataset.targets[:48000]\n",
    "\n",
    "#And the rest for validation\n",
    "validloader.dataset.data = validloader.dataset.data[48000:,:,:]\n",
    "validloader.dataset.targets = validloader.dataset.targets[48000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1626349041406,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "7-KYSulQrvYr"
   },
   "outputs": [],
   "source": [
    "class MLP_extended(MLP):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,hidden1,hidden2,nlabels=10,epochs=10,lr=0.001,use_batch_norm=True):\n",
    "        \n",
    "        super().__init__(dimx,hidden1,hidden2,nlabels,use_batch_norm)  #To initialize `MLP`!\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = [] \n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            self.train()\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:              \n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step()\n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            \n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad(): \n",
    "                \n",
    "                self.eval()\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    out = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "                    \n",
    "\n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f\" \n",
    "                      %(e + 1,self.loss_during_training[-1],self.valid_loss_during_training[-1]))\n",
    "\n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "\n",
    "                probs = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfoBFfLOr0cv"
   },
   "source": [
    "### Create two different models for testing\n",
    "\n",
    "* `net_batchnorm` uses batch normalization applied to the output of its hidden layers\n",
    "* `net_no_norm` does not use batch normalization\n",
    "\n",
    "Besides the normalization layers, everthing about these models is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1626349045974,
     "user": {
      "displayName": "PABLO SANCHEZ MARTIN",
      "photoUrl": "",
      "userId": "06191209259057108184"
     },
     "user_tz": -120
    },
    "id": "tLh0t3Npr0S1",
    "outputId": "85b30c08-7146-4ba7-f0c5-b76b4c6d65ff"
   },
   "outputs": [],
   "source": [
    "net_batchnorm = MLP_extended(dimx=784,hidden1=128,hidden2=64,\n",
    "                              nlabels=10,epochs=10,lr=1e-3,use_batch_norm=True)\n",
    "net_no_norm = MLP_extended(dimx=784,hidden1=128,hidden2=64,\n",
    "                              nlabels=10,epochs=10,lr=1e-3,use_batch_norm=False)\n",
    "\n",
    "print(net_batchnorm)\n",
    "print('-----------')\n",
    "print(net_no_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN6D1H6ir5bk"
   },
   "source": [
    "> **Exercise:** Train both models and compare the evolution of the train/validation loss in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FE5WtXxrxge"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "net_batchnorm.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBdSzNfIr8Zw"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "net_no_norm.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WYa5S4LsBDM"
   },
   "outputs": [],
   "source": [
    "# compare\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(net_batchnorm.loss_during_training, '-b', label=' Train (BN)')\n",
    "plt.plot(net_batchnorm.valid_loss_during_training,'--b', label=' Valid (BN)')\n",
    "plt.plot(net_no_norm.loss_during_training, '-r', label=' Train (Without BN)')\n",
    "plt.plot(net_no_norm.valid_loss_during_training,'--r', label=' Valid (Without BN)')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjhOUBPKsMRW"
   },
   "source": [
    "---\n",
    "### Considerations for other network types\n",
    "\n",
    "This notebook demonstrates batch normalization in a standard neural network with fully connected layers. You can also use batch normalization in other types of networks, but there are some special considerations.\n",
    "\n",
    "#### ConvNets\n",
    "\n",
    "Convolution layers consist of multiple feature maps. (Remember, the width of a convolutional layer refers to its number of feature maps.) And the weights for each feature map are shared across all the inputs that feed into the layer. Because of these differences, batch normalizing convolutional layers requires batch/population mean and variance per feature map rather than per node in the layer.\n",
    "\n",
    "> To apply batch normalization on the outputs of convolutional layers, we use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d). To use it, we simply state the **number of input feature maps**. I.e. `nn.BatchNorm2d(num_features=nmaps)`\n",
    "\n",
    "\n",
    "#### RNNs\n",
    "\n",
    "Batch normalization can work with recurrent neural networks, too, as shown in the 2016 paper [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025). It's a bit more work to implement, but basically involves calculating the means and variances per time step instead of per layer. You can find an example where someone implemented recurrent batch normalization in PyTorch, in [this GitHub repo](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBP2dkeQsSep"
   },
   "source": [
    "> **Exercise:** Using CIFAR 10 database, incorporate BN to your solution of Chapter I. Compare the results with and without BN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPzqloD_sMCK"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjXP_z2Wsipl"
   },
   "outputs": [],
   "source": [
    "validloader = copy.deepcopy(trainloader)  # Creates a copy of the object \n",
    "\n",
    "#We take the first 45k images for training\n",
    "trainloader.dataset.data = trainloader.dataset.data[:45000,:,:,:]\n",
    "trainloader.dataset.targets = trainloader.dataset.targets[:45000]\n",
    "\n",
    "#And the rest for validation\n",
    "validloader.dataset.data = validloader.dataset.data[45000:,:,:]\n",
    "validloader.dataset.targets = validloader.dataset.targets[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfwOFIqTtE5H"
   },
   "outputs": [],
   "source": [
    "class Lenet5_Drop_BN(nn.Module):\n",
    "    def __init__(self,dimx,nlabels,prob): #Nlabels will be 10 in our case\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional layer (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, \n",
    "                               kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # convolutional layer (sees 12x12x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)\n",
    "        \n",
    "        # Max pool layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = nn.Linear(400,120)\n",
    "        \n",
    "        self.linear2 = nn.Linear(120,84)\n",
    "        \n",
    "        self.linear3 = nn.Linear(84,10)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "        self.BN_1 = nn.BatchNorm2d(num_features=6)\n",
    "\n",
    "        self.BN_2 = nn.BatchNorm2d(num_features=16)\n",
    "\n",
    "        self.BN_3 = nn.BatchNorm1d(120)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        # Spatial dimension of the Tensor at the output of the 2nd CNN\n",
    "        self.final_dim = int(((dimx-4)/2-4)/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through the CNN operations\n",
    "        x = self.conv1(x) #YOUR CODE HERE\n",
    "        x = self.BN_1(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.pool(x)\n",
    "        #x = self.dropout(x) \n",
    "        x = self.conv2(x)\n",
    "        x = self.BN_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x) \n",
    "        # Flatten the tensor into a vector\n",
    "        x = x.view(-1, 16 * self.final_dim**2)\n",
    "        # Pass the tensor through the Dense Layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x) \n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.logsoftmax(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3_9lXFYtKLj"
   },
   "outputs": [],
   "source": [
    "class Lenet5_extended_GPU_Drop_BN(Lenet5_Drop_BN):\n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self,dimx,nlabels,prob,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,nlabels,prob)  \n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = []\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:\n",
    "                \n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device)  \n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images)\n",
    "\n",
    "                #Your code here\n",
    "                loss = self.criterion(out,labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #Your code here\n",
    "                loss.backward()\n",
    "                \n",
    "                #Your code here\n",
    "                self.optim.step()\n",
    "                \n",
    "                \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            \n",
    "            # Validation Loss\n",
    "            \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                self.eval()\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                for images,labels in validloader:\n",
    "                    \n",
    "                    # Move input and label tensors to the default device\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)                    \n",
    "                    \n",
    "                    out = self.forward(images)\n",
    "\n",
    "                    #Your code here\n",
    "                    loss = self.criterion(out,labels)\n",
    "\n",
    "                    running_loss += loss.item()   \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/len(validloader))    \n",
    "\n",
    "            # set model back to train mode\n",
    "            self.train()\n",
    "\n",
    "            if(e % 1 == 0): # Every 10 epochs\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f, Time per epoch: %f seconds\" \n",
    "                      %(e + 1,self.loss_during_training[-1],self.valid_loss_during_training[-1],\n",
    "                       (time.time() - start_time)))\n",
    "                \n",
    "    def eval_performance(self,dataloader):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        self.eval()\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in dataloader:\n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device) \n",
    "                probs = self.forward(images)\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "            self.train()\n",
    "            return accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "896b17adssHs"
   },
   "outputs": [],
   "source": [
    "my_CNN_GPU_Drop_BN = Lenet5_extended_GPU_Drop_BN(dimx=32,nlabels=10,prob=0.5,epochs=50,lr=1e-3)\n",
    "my_CNN_GPU_Drop_BN.trainloop(trainloader,validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FL1nry1s0E6"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_CNN_GPU_Drop_BN.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN_GPU_Drop_BN.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN_GPU_Drop_BN.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN_GPU_Drop_BN.eval_performance(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JknlffsEtiq-"
   },
   "outputs": [],
   "source": [
    "# my_CNN_GPU_Drop is trained in Chapter I\n",
    "\n",
    "plt.plot(my_CNN_GPU_Drop.loss_during_training,label='Training Loss')\n",
    "plt.plot(my_CNN_GPU_Drop.valid_loss_during_training,label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on train set: %.2f\" % my_CNN_GPU_Drop.eval_performance(trainloader))\n",
    "print(\"Accuracy on validation set: %.2f\" % my_CNN_GPU_Drop.eval_performance(validloader))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LAB_CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "056f77ec13a8494a939307d1dd9ccd94": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "084989368adb44b3a337749b1eb855c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7bf1a83c39f441a955eea59b16699ec",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc00377c262c4b7ab3d3bbbbd7c8bcd7",
      "value": 1648877
     }
    },
    "09ca9aa3b64d4f2aa3d33f45008383c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_056f77ec13a8494a939307d1dd9ccd94",
      "placeholder": "​",
      "style": "IPY_MODEL_a47a133f144a424d83375a32c7cb5636",
      "value": " 5120/? [00:00&lt;00:00, 120998.63it/s]"
     }
    },
    "10ebd6941c274e7f8584cf75a79b6f4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd8b2fdc2cdb46afa41cf3057dcc08d3",
      "placeholder": "​",
      "style": "IPY_MODEL_4c2c4f1597a3406dae7e8f92db7a28de",
      "value": " 9913344/? [04:32&lt;00:00, 36441.67it/s]"
     }
    },
    "12e82b5fc40a494e8d441b02ada487b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45e89804b91b4ca4ba1ba9eb61c30987",
       "IPY_MODEL_10ebd6941c274e7f8584cf75a79b6f4f"
      ],
      "layout": "IPY_MODEL_1824ce4041514aff9523f4e4fbea0163"
     }
    },
    "1824ce4041514aff9523f4e4fbea0163": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19a2f818ac9a4e66a00e281179081d5d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a1bbad96bef4540864980f74f767887": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "312521b46a2a4bb4bc7987f0d9b56aeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "316d38faeacc4dfea6c77bb8441039e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36641a81159e42da8bd64d75af6b907f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fcf45bc3193468d8a2938550b26999a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45e89804b91b4ca4ba1ba9eb61c30987": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36641a81159e42da8bd64d75af6b907f",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cb7bed1ba6b4f3188d4139d3b496b51",
      "value": 9912422
     }
    },
    "4c2c4f1597a3406dae7e8f92db7a28de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7125df1f44634c059d563601ad8219e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77050df183934b04a5ed583f87075837": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7125df1f44634c059d563601ad8219e2",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1a1bbad96bef4540864980f74f767887",
      "value": 4542
     }
    },
    "7cb7bed1ba6b4f3188d4139d3b496b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "836267d7b47e43149da45f05fedcf6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d275b58bf5d41099b10fa29e2cb89b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9d8cbf9151d34fb9833a155a4e78e651": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_084989368adb44b3a337749b1eb855c9",
       "IPY_MODEL_e5de62858f644f719a507050101a4a3d"
      ],
      "layout": "IPY_MODEL_c686fec736044af3a676b9ac879b2830"
     }
    },
    "9fc878b6c47b43fd824560215e6e54db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb20c5d14cfa4e6b9a4807222a55b7f5",
       "IPY_MODEL_fd1eb28cc4b141f58ec7e455d55685c1"
      ],
      "layout": "IPY_MODEL_b8aa27ae454140f6b7bb6fa5767985bf"
     }
    },
    "a070fec2b8ff4509a27bc1c9f2653a63": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a47a133f144a424d83375a32c7cb5636": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7bf1a83c39f441a955eea59b16699ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8aa27ae454140f6b7bb6fa5767985bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd8b2fdc2cdb46afa41cf3057dcc08d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c686fec736044af3a676b9ac879b2830": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb20c5d14cfa4e6b9a4807222a55b7f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_836267d7b47e43149da45f05fedcf6ee",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d275b58bf5d41099b10fa29e2cb89b0",
      "value": 28881
     }
    },
    "ce333287af5545b598496f5be757ecff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77050df183934b04a5ed583f87075837",
       "IPY_MODEL_09ca9aa3b64d4f2aa3d33f45008383c3"
      ],
      "layout": "IPY_MODEL_19a2f818ac9a4e66a00e281179081d5d"
     }
    },
    "e5de62858f644f719a507050101a4a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a070fec2b8ff4509a27bc1c9f2653a63",
      "placeholder": "​",
      "style": "IPY_MODEL_312521b46a2a4bb4bc7987f0d9b56aeb",
      "value": " 1649664/? [00:00&lt;00:00, 3441738.70it/s]"
     }
    },
    "fc00377c262c4b7ab3d3bbbbd7c8bcd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fd1eb28cc4b141f58ec7e455d55685c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_316d38faeacc4dfea6c77bb8441039e1",
      "placeholder": "​",
      "style": "IPY_MODEL_3fcf45bc3193468d8a2938550b26999a",
      "value": " 29696/? [00:01&lt;00:00, 27725.67it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
